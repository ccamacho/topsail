---
- name: Create the src directory
  file:
    path: "{{ artifact_extra_logs_dir }}/src"
    state: directory
    mode: '0755'

- name: Create the artifacts directory
  file:
    path: "{{ artifact_extra_logs_dir }}/artifacts"
    state: directory
    mode: '0755'

# Cleanup the namespace
- name: Delete the InferenceServices
  command: oc delete InferenceServices,ServingRuntime --all -n {{ kserve_deploy_model_namespace }}
  when: kserve_deploy_model_delete_others | bool

- name: Wait for the Pods to disappear
  command:
    oc get pods
       --no-headers
       -lcomponent=predictor
       -n {{ kserve_deploy_model_namespace }}
  register: ns_had_predictor_pods_cmd
  retries: 12
  delay: 10
  until: '"No resources found" in ns_had_predictor_pods_cmd.stderr'
  when: kserve_deploy_model_delete_others | bool

# SMMR

- name: Delete the tracking timestamps
  command:
    oc delete cm -ltopsail.time-tracking -n {{ kserve_deploy_model_namespace }}

- name: Save timestamp
  shell: |
    NAME=start-deploy-model
    oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
    oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

- name: Prepare the SMMR
  block:
  - name: Patch the SMMR
    command: |
      oc patch smmr/default \
         -n istio-system \
         --type=json \
         -p="[{'op': 'add', 'path': '/spec/members/-', 'value': \"{{ kserve_deploy_model_namespace }}\"}]"
    register: patch_smmr_cmd
    failed_when: false

  - name: Check that the namespace is already registered
    when: patch_smmr_cmd.rc != 0
    shell:
      set -o pipefail;
      oc get smmr/default -n istio-system  -ojsonpath={.spec.members} | jq .[] -r
    register: smmr_members_cmd
    failed_when: kserve_deploy_model_namespace not in smmr_members_cmd.stdout_lines

  - name: Wait for the namespace to be registered
    shell:
      set -o pipefail;
      oc get smmr/default
         -n istio-system
         -ojsonpath={.status.configuredMembers}
         | jq '. | index("{{ kserve_deploy_model_namespace }}")'
    register: smmr_registered_namespace_cmd
    retries: 60
    delay: 10
    until: smmr_registered_namespace_cmd.stdout != "null"

  - name: Save timestamp
    shell: |
      NAME=smmr-registered-namespace
      oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

  always:
  - name: Capture the SMMR resource
    shell:
      oc get smmr/default
         -n istio-system
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/istio-system_smmr-default.yaml

# Secret

- name: Prepare the secret parameters
  shell: |
    set -o pipefail;
    set -e

    cat "{{ kserve_deploy_model_secret_env_file_name }}" | sha256sum > {{ artifact_extra_logs_dir }}/artifacts/secret_env_file.sha256sum

    secret_data=$(cat "{{ kserve_deploy_model_secret_env_file_name }}" \
     | yq '.["{{ kserve_deploy_model_secret_env_file_key }}"] | to_entries | map( { key: .key, value: .value|tostring|@base64 }) | from_entries')

    if [[ -z "$secret_data" ]]; then
      echo "Could not find key '{{ kserve_deploy_model_secret_env_file_key }}' in file {{ kserve_deploy_model_secret_env_file_name }}. Aborting."
      exit 1
    fi
    oc create secret generic {{ kserve_deploy_model_serving_runtime_name }}-secret \
       -n {{ kserve_deploy_model_namespace }} \
       -ojson \
       --dry-run=client \
       | jq --argjson data "$secret_data" '.data = $data' \
       | oc apply -f-

    oc describe secret/{{ kserve_deploy_model_serving_runtime_name }}-secret \
       -n {{ kserve_deploy_model_namespace }} \
       > {{ artifact_extra_logs_dir }}/artifacts/env-secrets.desc

  when: kserve_deploy_model_secret_env_file_name != None

# Serving Runtime

- name: Prepare the caikit-tgis-config template
  template:
    src: "{{ caikit_tgit_config_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/caikit_tgit_config.yaml"
    mode: '0400'

- name: Prepare the ServingRuntime template
  template:
    src: "{{ serving_runtime_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
    mode: '0400'

- name: Create or update the Caikit TGIS config
  shell:
    set -o pipefail;

    oc create cm {{ kserve_deploy_model_serving_runtime_name }}-caikit-tgis-config
       -n {{ kserve_deploy_model_namespace }}
       --from-file=caikit.yml="{{ artifact_extra_logs_dir }}/src/caikit_tgit_config.yaml"
       --dry-run=client
       -oyaml
       | oc apply -f-

- name: Create the ServingRuntime
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"

# Inference Service

- name: Prepare the InferenceService template
  template:
    src: "{{ inference_service_template }}"
    dest: "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
    mode: '0400'

- name: Create the InferenceService
  command:
    oc apply -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"

- name: Prepare the InferenceService
  block:
  - name: Wait for the InferenceService Pod to appear
    command:
      oc get pod
      -oname
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_name
    # wait 15 minutes
    retries: 90
    delay: 10
    until: inference_service_pod_name.stdout | length > 0

  - name: Inform about the next task
    debug:
      msg: |
        The next tasks wait for loading of the InferenceService Pod
        Watch the progress with this command: oc get pods -n {{ kserve_deploy_model_namespace }} -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}

  - name: Wait for the InferenceService Pod to be scheduled
    command:
      oc get pod
      -ojsonpath={.items[0].spec.nodeName}
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_nodename
    # wait 1 minutes
    retries: 6
    delay: 10
    until: inference_service_pod_nodename.stdout | length > 0

  - name: Wait for the InferenceService Pod to be fetch the model from S3
    command:
      oc get pod
      -ojsonpath={.items[0].status.initContainerStatuses[0].state.terminated}
      -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
      -n {{ kserve_deploy_model_namespace }}
    register: inference_service_pod_fetching_cmd
    # wait 60 minutes
    retries: 120
    delay: 30
    until: inference_service_pod_fetching_cmd.stdout | length > 0

  - name: Wait for all the containers to be ready
    shell: |
      set -o pipefail;
      set -e;

      status=$(oc get pod \
        -ojson \
        -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} \
        -n {{ kserve_deploy_model_namespace }} \
        | jq -r '.items[0].status.containerStatuses[] | ("" + .name +" ready="+ (.ready|tostring)) +" restarted="+(.restartCount|tostring)')
      echo "$status"

      if grep -v restarted=0 --silent <<< $status; then
        echo "Container restart detected, aborting" >&2
        exit 2
      fi

      if grep -v ready=true --silent <<< $status; then
        echo "Containers not ready detected, keep waiting ..." >&2
        exit 1
      fi

      echo "All the containers are ready, all good :)" >&2
    register: inference_service_pod_ready
    failed_when: inference_service_pod_ready.rc == 2
    until: inference_service_pod_ready.rc == 0 or inference_service_pod_ready.rc == 2
    # wait 90 minutes
    retries: 180
    delay: 30

  - name: Wait for the InferenceService Pod to initialize the model
    shell:
      set -o pipefail;
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
         -ojsonpath={.status.modelStatus.states.targetModelState}
    register: inference_service_state_cmd
    # wait 5 minutes
    retries: 30
    delay: 10
    until: inference_service_state_cmd.stdout == "Loaded"

  - name: Capture the state of the InferenceService Pod resource
    shell:
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide

  - name: Save timestamp
    shell: |
      NAME=inference-service-loaded
      oc create configmap $NAME -n {{ kserve_deploy_model_namespace }}
      oc label cm/$NAME topsail.time-tracking=yes -n {{ kserve_deploy_model_namespace }}

  always:
  - name: Capture the state of the InferenceService Pod resource
    shell:
      set -o pipefail;

      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -oyaml
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.yaml;
      oc get pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -owide
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.status;
      oc describe pod
         -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }}
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.desc

      oc logs $(oc get pod -lserving.kserve.io/inferenceservice={{ kserve_deploy_model_inference_service_name }} -n {{ kserve_deploy_model_namespace }} -oname | head -1)
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/predictor-pod.log
    ignore_errors: true

  - name: Capture the state of the InferenceService resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/inference_service.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/inference_service.yaml

  - name: Capture the state of the ServingRuntime resource
    shell:
      oc get -f "{{ artifact_extra_logs_dir }}/src/serving_runtime.yaml"
         -oyaml
         > {{ artifact_extra_logs_dir }}/artifacts/serving_runtime.yaml

  - name: Save the timestamp configmaps
    shell:
      oc get cm -oyaml
         -ltopsail.time-tracking=yes
         -n {{ kserve_deploy_model_namespace }}
         > {{ artifact_extra_logs_dir }}/artifacts/time_tracking_cm.yaml
